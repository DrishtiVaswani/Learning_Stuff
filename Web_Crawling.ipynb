{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psOXRAyQQgJL",
        "outputId": "2693320d-f011-4d16-ecda-ca147ab07836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "ah5vGQKCQrZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install LSI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXV0aaVDQ4GZ",
        "outputId": "7d70014c-1f2c-4ce8-c5a4-3287e5b7b673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: LSI in /usr/local/lib/python3.11/dist-packages (0.4.0)\n",
            "Requirement already satisfied: boto>=2.32.1 in /usr/local/lib/python3.11/dist-packages (from LSI) (2.49.0)\n",
            "Requirement already satisfied: colored>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from LSI) (2.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from LSI import LSI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "yzsNdca4Q9Vs",
        "outputId": "d5442af8-c564-4826-b791-69676d057b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'LSI'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9060ae501941>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mLSI\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'LSI'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import time\n",
        "import os\n",
        "from IPython.display import display, HTML\n",
        "from bs4 import XMLParsedAsHTMLWarning\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
        "\n",
        "class WebCrawler:\n",
        "    def __init__(self, url, depth=1, delay=1):\n",
        "        \"\"\"\n",
        "        Initialize the web crawler\n",
        "\n",
        "        Args:\n",
        "            url (str): The starting URL to crawl\n",
        "            depth (int): How many levels of links to follow (default: 1)\n",
        "            delay (int): Delay between requests in seconds (default: 1)\n",
        "        \"\"\"\n",
        "        self.starting_url = url\n",
        "        self.depth = depth\n",
        "        self.delay = delay\n",
        "        self.visited_urls = set()\n",
        "        self.data = []\n",
        "        self.headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "\n",
        "    def is_valid_url(self, url):\n",
        "        \"\"\"Check if url is valid and has the same domain as the starting url\"\"\"\n",
        "        try:\n",
        "            parsed_url = urlparse(url)\n",
        "            starting_parsed = urlparse(self.starting_url)\n",
        "            return bool(parsed_url.netloc) and parsed_url.netloc == starting_parsed.netloc\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def extract_metadata(self, soup):\n",
        "        \"\"\"Extract metadata from the page\"\"\"\n",
        "        metadata = {}\n",
        "\n",
        "        # Extract meta tags\n",
        "        for meta in soup.find_all('meta'):\n",
        "            if meta.get('name'):\n",
        "                metadata[meta.get('name')] = meta.get('content')\n",
        "            elif meta.get('property'):\n",
        "                metadata[meta.get('property')] = meta.get('content')\n",
        "\n",
        "        description = soup.find('meta', attrs={'name': 'description'})\n",
        "        if description:\n",
        "            metadata['description'] = description.get('content')\n",
        "\n",
        "        for meta in soup.find_all('meta', attrs={'property': re.compile(r'^og:')}):\n",
        "            metadata[meta.get('property')] = meta.get('content')\n",
        "\n",
        "        for meta in soup.find_all('meta', attrs={'name': re.compile(r'^twitter:')}):\n",
        "            metadata[meta.get('name')] = meta.get('content')\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    def crawl(self, url, current_depth=0):\n",
        "        \"\"\"\n",
        "        Crawl the webpage and extract data\n",
        "\n",
        "        Args:\n",
        "            url (str): URL to crawl\n",
        "            current_depth (int): Current depth level\n",
        "        \"\"\"\n",
        "        if url in self.visited_urls or current_depth > self.depth:\n",
        "            return\n",
        "\n",
        "        self.visited_urls.add(url)\n",
        "\n",
        "        try:\n",
        "            print(f\"Crawling: {url}\")\n",
        "            response = requests.get(url, headers=self.headers, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            title = soup.title.string if soup.title else \"No Title\"\n",
        "\n",
        "            metadata = self.extract_metadata(soup)\n",
        "\n",
        "            links = []\n",
        "            for a_tag in soup.find_all('a', href=True):\n",
        "                link = a_tag.get('href')\n",
        "                text = a_tag.text.strip()\n",
        "\n",
        "                absolute_link = urljoin(url, link)\n",
        "\n",
        "                link_data = {\n",
        "                    'text': text,\n",
        "                    'url': absolute_link\n",
        "                }\n",
        "                links.append(link_data)\n",
        "\n",
        "                page_data = {\n",
        "                    'url': url,\n",
        "                    'title': title,\n",
        "                    'metadata': metadata,\n",
        "                    'links': links\n",
        "                }\n",
        "\n",
        "                for i, item in enumerate(self.data):\n",
        "                    if item['url'] == url:\n",
        "                        self.data[i] = page_data\n",
        "                        break\n",
        "                else:\n",
        "                    self.data.append(page_data)\n",
        "\n",
        "                if current_depth < self.depth and self.is_valid_url(absolute_link):\n",
        "                    time.sleep(self.delay)\n",
        "                    self.crawl(absolute_link, current_depth + 1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error crawling {url}: {str(e)}\")\n",
        "\n",
        "    def start_crawling(self):\n",
        "        \"\"\"Start the crawling process from the starting URL\"\"\"\n",
        "        self.crawl(self.starting_url)\n",
        "        return self.data\n",
        "\n",
        "    def save_as_json(self, filename=\"crawled_data.json\"):\n",
        "        \"\"\"Save crawled data as JSON\"\"\"\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.data, f, indent=4, ensure_ascii=False)\n",
        "        print(f\"Data saved to {filename}\")\n",
        "\n",
        "    def save_as_csv(self, filename=\"crawled_data.csv\"):\n",
        "        \"\"\"Save crawled data as CSV (flattened structure)\"\"\"\n",
        "        flattened_data = []\n",
        "\n",
        "        for page in self.data:\n",
        "            page_info = {\n",
        "                'url': page['url'],\n",
        "                'title': page['title']\n",
        "            }\n",
        "\n",
        "            for key, value in page['metadata'].items():\n",
        "                page_info[f'meta_{key}'] = value\n",
        "\n",
        "            if not page['links']:\n",
        "                flattened_data.append(page_info)\n",
        "            else:\n",
        "                for link in page['links']:\n",
        "                    link_info = page_info.copy()\n",
        "                    link_info['link_text'] = link['text']\n",
        "                    link_info['link_url'] = link['url']\n",
        "                    flattened_data.append(link_info)\n",
        "\n",
        "        df = pd.DataFrame(flattened_data)\n",
        "        df.to_csv(filename, index=False, encoding='utf-8')\n",
        "        print(f\"Data saved to {filename}\")\n",
        "\n",
        "    def display_summary(self):\n",
        "        \"\"\"Display a summary of the crawled data\"\"\"\n",
        "        num_pages = len(self.data)\n",
        "        num_links = sum(len(page['links']) for page in self.data)\n",
        "\n",
        "        print(f\"\\nCrawling Summary:\")\n",
        "        print(f\"----------------\")\n",
        "        print(f\"Starting URL: {self.starting_url}\")\n",
        "        print(f\"Depth: {self.depth}\")\n",
        "        print(f\"Pages crawled: {num_pages}\")\n",
        "        print(f\"Total links found: {num_links}\")\n",
        "        print(f\"Unique URLs visited: {len(self.visited_urls)}\")\n",
        "\n",
        "url = \"https://web-scraping.dev/\"\n",
        "crawler = WebCrawler(url, depth=1, delay=1)\n",
        "data = crawler.start_crawling()\n",
        "crawler.save_as_json()\n",
        "crawler.save_as_csv()\n",
        "crawler.display_summary()\n",
        "\n",
        "if data:\n",
        "    print(\"\\nPreview of the first page data:\")\n",
        "    print(f\"URL: {data[0]['url']}\")\n",
        "    print(f\"Title: {data[0]['title']}\")\n",
        "    print(f\"Metadata: {json.dumps(data[0]['metadata'], indent=2)}\")\n",
        "    print(f\"Links found: {len(data[0]['links'])}\")\n",
        "    if data[0]['links']:\n",
        "        print(f\"First link: {data[0]['links'][0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stLJKwbuXJqY",
        "outputId": "f292101d-d69c-4ee0-bdcb-eec1a09c0030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling: https://web-scraping.dev/\n",
            "Crawling: https://web-scraping.dev/docs\n",
            "Crawling: https://web-scraping.dev/api/graphql\n",
            "Crawling: https://web-scraping.dev/products\n",
            "Crawling: https://web-scraping.dev/reviews\n",
            "Crawling: https://web-scraping.dev/testimonials\n",
            "Crawling: https://web-scraping.dev/login\n",
            "Crawling: https://web-scraping.dev/cart\n",
            "Crawling: https://web-scraping.dev/#scenarios\n",
            "Crawling: https://web-scraping.dev/product/1\n",
            "Crawling: https://web-scraping.dev/login?cookies=\n",
            "Crawling: https://web-scraping.dev/blocked\n",
            "Crawling: https://web-scraping.dev/credentials\n",
            "Crawling: https://web-scraping.dev/blocked?persist=\n",
            "Crawling: https://web-scraping.dev/sitemap.xml\n",
            "Crawling: https://web-scraping.dev/robots.txt\n",
            "Data saved to crawled_data.json\n",
            "Data saved to crawled_data.csv\n",
            "\n",
            "Crawling Summary:\n",
            "----------------\n",
            "Starting URL: https://web-scraping.dev/\n",
            "Depth: 1\n",
            "Pages crawled: 12\n",
            "Total links found: 217\n",
            "Unique URLs visited: 16\n",
            "\n",
            "Preview of the first page data:\n",
            "URL: https://web-scraping.dev/\n",
            "Title: web-scraping.dev\n",
            "Metadata: {\n",
            "  \"viewport\": \"width=device-width, initial-scale=1, shrink-to-fit=no\",\n",
            "  \"description\": \"Mock e-commerce website web scraping testing and exploring popular webdev patterns\",\n",
            "  \"og:site_name\": \"Scrapeground\",\n",
            "  \"og:type\": \"article\",\n",
            "  \"og:title\": \"web-scraping.dev\",\n",
            "  \"og:description\": \"Mock e-commerce website web scraping testing and exploring popular webdev patterns\",\n",
            "  \"og:url\": \"https://web-scraping.dev/\",\n",
            "  \"og:image\": \"https://web-scraping.dev/banners/index.png\",\n",
            "  \"og:image:type\": \"image/png\",\n",
            "  \"article:published_time\": \"2023-05-20T00:00:00.000Z\",\n",
            "  \"article:modified_time\": \"2023-05-20T00:00:00.000Z\",\n",
            "  \"article:tag\": \"webscraping,mock\",\n",
            "  \"article:publisher\": \"https://web-scraping.dev/\",\n",
            "  \"twitter:card\": \"summary_large_image\",\n",
            "  \"twitter:title\": \"web-scraping.dev\",\n",
            "  \"twitter:description\": \"Mock e-commerce website web scraping testing and exploring popular webdev patterns\",\n",
            "  \"twitter:url\": \"https://web-scraping.dev/\",\n",
            "  \"twitter:image\": \"https://web-scraping.dev/banners/index.png\",\n",
            "  \"twitter:site\": \"@Scrapfly_dev\"\n",
            "}\n",
            "Links found: 47\n",
            "First link: {'text': 'web-scraping.dev', 'url': 'https://web-scraping.dev/'}\n"
          ]
        }
      ]
    }
  ]
}